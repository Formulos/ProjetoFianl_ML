Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
10000,1.4189383,-1.4566169,125.44155844155844,-0.987012987012987,-0.987012987012987,1.0
20000,1.4182388,1.2745103,143.52941176470588,-0.9701492537313433,-0.9701492537313433,1.0
30000,1.419079,2.0914319,150.5820895522388,-1.0,-1.0,1.0
40000,1.4197488,3.2964964,130.75342465753425,-0.9863013698630136,-0.9863013698630136,1.0
50000,1.4191557,3.417841,135.41558441558442,-0.974025974025974,-0.974025974025974,1.0
60000,1.417882,3.6282046,130.13698630136986,-0.9864864864864865,-0.9864864864864865,1.0
70000,1.4171486,3.5495923,132.62666666666667,-1.0,-1.0,1.0
80000,1.4171375,3.8156989,140.73239436619718,-1.0,-1.0,1.0
90000,1.4179965,3.8515596,135.05405405405406,-1.0,-1.0,1.0
100000,1.4179531,3.9393253,138.11111111111111,-0.9863013698630136,-0.9863013698630136,1.0
110000,1.4191166,3.7541144,138.66197183098592,-1.0,-1.0,1.0
120000,1.420807,3.5380232,128.2027027027027,-1.0,-1.0,1.0
130000,1.4213649,3.2418573,139.24324324324326,-0.9863013698630136,-0.9863013698630136,1.0
140000,1.4217277,2.9505808,127.1470588235294,-1.0,-1.0,1.0
150000,1.4224823,2.52872,160.66666666666666,-1.0,-1.0,1.0
160000,1.4239892,2.4875336,161.625,-0.9682539682539683,-0.9682539682539683,1.0
170000,1.4270658,3.0685022,117.71604938271605,-1.0,-1.0,1.0
180000,1.4292159,2.7990978,110.38636363636364,-0.9886363636363636,-0.9886363636363636,1.0
