Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
10000,1.4189383,-3.8689566,98.03960396039604,-0.96,-0.96,1.0
20000,1.4206715,-2.2289426,103.67368421052632,-1.0,-1.0,1.0
30000,1.4200208,-1.1811597,96.66990291262135,-0.9509803921568627,-0.9509803921568627,1.0
40000,1.4212241,0.28520903,86.14035087719299,-1.0,-1.0,1.0
50000,1.4210857,0.14176089,86.33043478260869,-1.0,-1.0,1.0
60000,1.4203242,0.002730066,84.24786324786325,-1.0,-1.0,1.0
70000,1.4190273,-0.18875553,83.55932203389831,-1.0,-1.0,1.0
80000,1.4175146,-0.31959617,83.47899159663865,-1.0,-1.0,1.0
90000,1.4164541,-0.43806124,82.6890756302521,-1.0,-1.0,1.0
100000,1.4174294,-0.5309516,83.4873949579832,-1.0,-1.0,1.0
110000,1.4183273,-0.6024835,86.64035087719299,-1.0,-1.0,1.0
120000,1.4194843,-0.6646195,85.96521739130435,-1.0,-1.0,1.0
130000,1.4203389,-0.6874408,86.21052631578948,-1.0,-1.0,1.0
140000,1.4208814,-0.67222595,92.4074074074074,-1.0,-1.0,1.0
150000,1.4190764,-0.93102616,97.48514851485149,-0.9607843137254902,-0.9607843137254902,1.0
160000,1.41778,-1.0838231,86.70175438596492,-0.9736842105263158,-0.9736842105263158,1.0
170000,1.4167696,-0.827997,87.52212389380531,-1.0,-1.0,1.0
