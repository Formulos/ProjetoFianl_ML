Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
120000,1.4160742,0.15135399,467.2857142857143,1.0476190476190477,1.0476190476190477,1.0
130000,1.4175104,-0.3430529,561.1764705882352,1.2647058823529411,1.2647058823529411,1.0
140000,1.4172386,0.5456463,924.4545454545455,2.3181818181818183,2.3181818181818183,1.0
150000,1.4173841,0.119868375,904.3,1.85,1.85,1.0
160000,1.4176275,-0.12135317,834.3846153846154,2.3076923076923075,2.3076923076923075,1.0
170000,1.4173126,-0.13992703,769.7692307692307,1.5384615384615385,1.5384615384615385,1.0
180000,1.417054,-0.30930936,1527.25,3.5,3.5,1.0
190000,1.416345,1.5534972,1286.909090909091,3.272727272727273,3.272727272727273,1.0
200000,1.4151806,0.25611228,847.1818181818181,2.409090909090909,2.409090909090909,1.0
210000,1.4131227,-0.599822,982.3636363636364,2.1818181818181817,2.1818181818181817,1.0
220000,1.411074,-0.5590726,1496.6666666666667,2.6666666666666665,2.6666666666666665,1.0
230000,1.4095577,0.020955276,810.0769230769231,2.3461538461538463,2.3461538461538463,1.0
240000,1.4082577,0.094529495,1322.875,3.0714285714285716,3.0714285714285716,1.0
250000,1.4069638,-0.2894535,546.6666666666666,1.59375,1.59375,1.0
260000,1.4059558,-0.45126817,831.6428571428571,2.25,2.25,1.0
270000,1.4058974,-0.24680065,988.6666666666666,2.7777777777777777,2.7777777777777777,1.0
280000,1.4054976,0.058398787,983.3,2.6,2.6,1.0
290000,1.4050013,0.4188907,679.4666666666667,1.6666666666666667,1.6666666666666667,1.0
