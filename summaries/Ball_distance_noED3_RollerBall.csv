Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
10000,1.4189383,17.099743,27.444444444444443,-0.9629629629629629,-0.9629629629629629,1.0
20000,1.4162713,16.023066,28.3049853372434,-0.9413489736070382,-0.9413489736070382,1.0
30000,1.4184564,15.716779,27.335227272727273,-0.9431818181818182,-0.9431818181818182,1.0
40000,1.4219782,15.641245,28.49266862170088,-0.9294117647058824,-0.9294117647058824,1.0
50000,1.4251506,15.846194,29.19756838905775,-0.918429003021148,-0.918429003021148,1.0
60000,1.4291344,15.398847,27.74,-0.9398280802292264,-0.9398280802292264,1.0
70000,1.4329603,13.415923,27.982558139534884,-0.9335260115606936,-0.9335260115606936,1.0
80000,1.4359902,11.884087,28.43952802359882,-0.9557522123893806,-0.9557522123893806,1.0
90000,1.438344,12.750006,26.34153005464481,-0.9316939890710383,-0.9316939890710383,1.0
100000,1.4417173,11.384681,26.83565459610028,-0.9415041782729805,-0.9415041782729805,1.0
110000,1.4435958,10.910552,27.79310344827586,-0.9568965517241379,-0.9568965517241379,1.0
