Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
10000,1.4189383,-0.19669428,54.50555555555555,-4.110847029096072,-4.110847029096072,1.0
20000,1.4190311,-0.16225329,43.19383259911894,-2.726850050255173,-2.726850050255173,1.0
30000,1.4201225,-0.19613735,42.35930735930736,-2.6131715816518724,-2.6131715816518724,1.0
40000,1.4213172,-0.46265608,36.80228136882129,-2.315916461909486,-2.315916461909486,1.0
50000,1.4229374,-0.5744831,38.13725490196079,-2.636155504641333,-2.636155504641333,1.0
60000,1.4217751,-0.5606683,34.670212765957444,-1.9726062469544106,-1.9726062469544106,1.0
70000,1.4222934,-0.67705566,34.29681978798587,-1.8559007907105596,-1.8559007907105596,1.0
80000,1.4240547,-0.68734443,32.40133779264214,-1.6417039170202528,-1.6417039170202528,1.0
90000,1.4247267,-0.8247827,32.51178451178451,-1.806399689929192,-1.806399689929192,1.0
100000,1.4254411,-0.8548409,31.028662420382165,-1.656038324559329,-1.656038324559329,1.0
110000,1.4259886,-0.8957671,29.73926380368098,-1.577006069825242,-1.577006069825242,1.0
