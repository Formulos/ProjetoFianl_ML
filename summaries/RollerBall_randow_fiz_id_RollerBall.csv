Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
10000,1.4189383,1.560326,134.54929577464787,-0.44366197183098594,-0.44366197183098594,1.0
20000,1.4205343,1.531506,120.44578313253012,-0.46987951807228917,-0.46987951807228917,1.0
30000,1.4208663,1.8294779,105.28421052631579,-0.4368421052631579,-0.4368421052631579,1.0
40000,1.4226296,1.9321007,94.23364485981308,-0.4532710280373832,-0.4532710280373832,1.0
50000,1.4245975,1.7198448,84.52136752136752,-0.47435897435897434,-0.47435897435897434,1.0
60000,1.4283568,2.0109253,84.04273504273505,-0.43162393162393164,-0.43162393162393164,1.0
70000,1.4313442,2.220086,73.9172932330827,-0.42857142857142855,-0.42857142857142855,1.0
80000,1.4325322,1.9907056,74.81203007518798,-0.4774436090225564,-0.4774436090225564,1.0
90000,1.43562,2.1558185,70.1418439716312,-0.4642857142857143,-0.4642857142857143,1.0
100000,1.4378558,2.0626242,66.4391891891892,-0.4527027027027027,-0.4527027027027027,1.0
110000,1.4393985,2.2781684,62.445859872611464,-0.4588607594936709,-0.4588607594936709,1.0
120000,1.4413488,2.1949167,62.63057324840764,-0.5,-0.5,1.0
130000,1.4426804,1.9902898,58.404761904761905,-0.47619047619047616,-0.47619047619047616,1.0
140000,1.4437789,1.9210243,56.724137931034484,-0.4454022988505747,-0.4454022988505747,1.0
